{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 中文"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import logging\n",
    "import collections\n",
    "import re\n",
    "import math\n",
    "import jieba\n",
    "from wordcloud import WordCloud\n",
    "jieba.setLogLevel(logging.INFO)\n",
    "import os\n",
    "\n",
    "root_path = '/Users/zhongshannan/Documents/fakenews_detection/dataset/weibo数据集处理/'\n",
    "\n",
    "def stopwordslist():\n",
    "    stopwords = [line.strip() for line in open(os.path.join(root_path, 'stop_words.txt'), encoding='UTF-8').readlines()]\n",
    "    stopwords = [k for s in stopwords for k in s]\n",
    "    return stopwords\n",
    "\n",
    "f = open(os.path.join(root_path, '事件29.txt'), \"r\", encoding=\"utf-8\")\n",
    "text = f.read()\n",
    "f.close()\n",
    "\n",
    "TopWordFrequencyPercentage = 20\n",
    "TopWordFrequencyPercentage /= 100\n",
    "\n",
    "# jieba分词\n",
    "seg = jieba.cut(text)\n",
    "seg = \" \".join(seg)\n",
    "seg = seg.strip()\n",
    "seg = re.sub(r\"[0-9\\s+\\.\\!\\/_,$%^*()?;；:-【】+\\\"\\']+|[+——！，;:：。？、~@#￥%……&*（）]+\", \" \", seg)\n",
    "seg = seg.split(\" \")\n",
    "seg = list(filter(None, seg))\n",
    "stopwords = stopwordslist()\n",
    "seg = [v for v in seg if v not in stopwords]\n",
    "word_counts = collections.Counter(seg)\n",
    "\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "colors = [\"#000000\", \"#111111\", \"#101010\", \"#121212\", \"#212121\", \"#222222\"]\n",
    "cmap = LinearSegmentedColormap.from_list(\"mycmap\", colors)\n",
    "\n",
    "# 生成词云\n",
    "wc = WordCloud(\n",
    "    max_words=math.ceil(len(word_counts) * TopWordFrequencyPercentage),\n",
    "    width=500,\n",
    "    height=350,\n",
    "    max_font_size=50,\n",
    "    min_font_size=10,\n",
    "    font_path='/System/Library/Fonts/Supplemental/Songti.ttc',\n",
    "    background_color='white',\n",
    "    colormap=cmap,\n",
    ")\n",
    "\n",
    "wc.generate_from_frequencies(word_counts)\n",
    "\n",
    "PicSavePath = \"事件29.png\"\n",
    "wc.to_file(PicSavePath)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x7fc5512c0a30>"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 英文"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import seaborn as sns\n",
    "\n",
    "trainTweets = pd.read_csv(\"/Users/zhongshannan/Documents/fakenews_detection/dataset/twitter/twitter_cleaned/train_posts.csv\")\n",
    "\n",
    "# 转小写\n",
    "trainTweets['post_text'] = trainTweets['post_text'].apply(lambda x: x.lower())\n",
    "trainTweets.head()\n",
    "\n",
    "# 删除标点\n",
    "def punctuation_removal(text):\n",
    "    all_list = [char for char in text if char not in string.punctuation]\n",
    "    clean_str = ''.join(all_list)\n",
    "    return clean_str\n",
    "\n",
    "trainTweets['post_text'] = trainTweets['post_text'].apply(punctuation_removal)\n",
    "trainTweets.head()\n",
    "\n",
    "# 删除英文停用词\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "trainTweets['post_text'] = trainTweets['post_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "\n",
    "\n",
    "en_stop_words = nltk.corpus.stopwords.words('english')\n",
    "sp_stop_words = nltk.corpus.stopwords.words('spanish')\n",
    "twitter = ['rt', 'http', 'co','https','nhttp','nhttps']\n",
    "\n",
    "# 完整停用词 由英文停用词、西班牙文停用词、事件词汇、rt构成\n",
    "block_words = en_stop_words + sp_stop_words + twitter\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 自定义图层颜色\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "colors = [\"#000000\", \"#111111\", \"#101010\", \"#121212\", \"#212121\", \"#222222\"]\n",
    "cmap = LinearSegmentedColormap.from_list(\"mycmap\", colors)\n",
    "\n",
    "def generate_word_cloud(input_tweets):\n",
    "    tweets = input_tweets\n",
    "    tweets_text = \" \".join(tweets.post_text.to_numpy().tolist())\n",
    "    \n",
    "    # 词云\n",
    "    wordcloud = WordCloud(width = 3000, \n",
    "                          height = 2000, \n",
    "                          random_state=1, \n",
    "                          background_color='White',\n",
    "                          colormap=cmap, \n",
    "                          collocations=False, \n",
    "                          font_path='/System/Library/Fonts/Supplemental/Marion.ttc',\n",
    "                          stopwords = block_words).generate(tweets_text)\n",
    "    # plt.figure(figsize=(16,10))\n",
    "    # plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    # plt.axis('off') # 关闭轴\n",
    "    # plt.show()\n",
    "    wordcloud.to_file(\"7.png\")\n",
    "\n",
    "generate_word_cloud(trainTweets[trainTweets['label'] == 'real'])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/zhongshannan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('Py38': conda)"
  },
  "interpreter": {
   "hash": "62792cbd3b5bf2807c49140b9dd77f3574e6f3863eef786f6f586b1ba9747cfd"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}