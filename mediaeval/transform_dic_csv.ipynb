{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import pickle    \n",
    "import pandas as pd\n",
    "test = pickle.load(open('/Users/zhongshannan/Desktop/train.pickle', 'rb'))\n",
    "\n",
    "for i in range(len(test['text'])):\n",
    "    test['text'][i] = test['text'][i][:-1]\n",
    "    #test['image'][i] = test['image'][i][0][:-4]  # 取到jpg\n",
    "    test['image'][i] = test['image'][i][:] # 全取\n",
    "    test['label'][i] = \"fake\" if test['label'][i] == 0 else \"real\" # label从01转换fake real\n",
    "    test['event'][i] = test['event'][i]\n",
    "\n",
    "test1 = pd.DataFrame(columns = ['text'] , data=test['text'])\n",
    "test2 = pd.DataFrame(columns = ['image'] , data=test['image'])\n",
    "test3 = pd.DataFrame(columns = ['label'] , data=test['label'])\n",
    "test4 = pd.DataFrame(columns = ['event_label'] , data=test['event'])\n",
    "df = pd.concat([test1,test2,test3,test4], axis=1)\n",
    "df.to_csv(\"/Users/zhongshannan/Documents/fakenews_detection/dataset/twitter_recon/twitter_cleaned/weibo_train.csv\", encoding='utf-8', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# twitter mvae处理"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "test = pickle.load(open('/Users/zhongshannan/Documents/fakenews_detection/dataset/twitter_recon/twitter_mvae/train.pickle', 'rb'))\n",
    "sum(test['label'])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "5188"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "test = pickle.load(open('/Users/zhongshannan/Documents/fakenews_detection/dataset/twitter_recon/twitter_mvae/test.pickle', 'rb'))\n",
    "\n",
    "for i in range(len(test['text'])):\n",
    "    test['text'][i] = test['text'][i][:-1]\n",
    "    #test['image'][i] = test['image'][i][0][:-4]  # 取到jpg\n",
    "    test['image'][i] = test['image'][i][0][:] + '.jpg' # 全取\n",
    "    test['label'][i] = \"fake\" if test['label'][i] == 0 else \"real\" # label从01转换fake real\n",
    "    test['event'][i] = test['event'][i]\n",
    "\n",
    "test1 = pd.DataFrame(columns = ['text'] , data=test['text'])\n",
    "test2 = pd.DataFrame(columns = ['image'] , data=test['image'])\n",
    "test3 = pd.DataFrame(columns = ['label'] , data=test['label'])\n",
    "test4 = pd.DataFrame(columns = ['event_label'] , data=test['event'])\n",
    "df = pd.concat([test1,test2,test3,test4], axis=1)\n",
    "df.to_csv(\"/Users/zhongshannan/Documents/fakenews_detection/dataset/twitter_recon/twitter_mvae/weibo_test.csv\", encoding='utf-8', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "检查验证集事件"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "import pandas as pd\n",
    "file = pd.read_csv('/Users/zhongshannan/Documents/fakenews_detection/dataset/twitter_recon/twitter_mvae/weibo_validation.csv')\n",
    "\n",
    "images=[]\n",
    "for image_name in file['image']:\n",
    "    images.append(image_name[:-7]) \n",
    "set(images),print(file['event_label'].unique())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[39 38 40 37]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "({'eclipse', 'garissa', 'nepal', 'samurai'}, None)"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "import pandas as pd\n",
    "file = pd.read_csv('/Users/zhongshannan/Documents/fakenews_detection/dataset/twitter_recon/twitter_mvae/weibo_test.csv')\n",
    "\n",
    "images=[]\n",
    "for image_name in file['image']:\n",
    "    images.append(image_name[:-7]) \n",
    "set(images),print(len(file['event_label'].unique()))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "25\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "({'attacks_pari',\n",
       "  'attacks_paris',\n",
       "  'black_lio',\n",
       "  'bowie_davi',\n",
       "  'brussels_explosion',\n",
       "  'burst_kf',\n",
       "  'bush_boo',\n",
       "  'five_headed_snak',\n",
       "  'fuji_lenticula',\n",
       "  'gandhi_dancin',\n",
       "  'half_everythin',\n",
       "  'hubble_telescop',\n",
       "  'immigrant',\n",
       "  'john_guevar',\n",
       "  'mc_donalds_fe',\n",
       "  'nazi_submarin',\n",
       "  'north_kore',\n",
       "  'not_afrai',\n",
       "  'pakistan_explosio',\n",
       "  'protes',\n",
       "  'refugee',\n",
       "  'refugees',\n",
       "  'rio_moo',\n",
       "  'soldier_stealin',\n",
       "  'syrian_childre',\n",
       "  'ukrainian_naz',\n",
       "  'woman_14_childre'},\n",
       " None)"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\n",
    "file = pd.read_csv('/Users/zhongshannan/Documents/fakenews_detection/dataset/twitter_recon/twitter_mvae/weibo_train.csv')\n",
    "\n",
    "images=[]\n",
    "for image_name in file['image']:\n",
    "    images.append(image_name[:-7]) \n",
    "set(images),print(len(file['event_label'].unique()))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "11\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "({'boston_fake',\n",
       "  'boston_real',\n",
       "  'bringback_fake',\n",
       "  'columbianChemicals_fake',\n",
       "  'elephant_fake',\n",
       "  'livr',\n",
       "  'malaysia_fake',\n",
       "  'passport',\n",
       "  'pigFish',\n",
       "  'sandy_fake',\n",
       "  'sandy_real',\n",
       "  'sandy_real_',\n",
       "  'sochi_fak',\n",
       "  'sochi_fake',\n",
       "  'underwater_fake'},\n",
       " None)"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import pandas as pd\n",
    "file = pd.read_csv('/Users/zhongshannan/Documents/fakenews_detection/dataset/twitter_recon/twitter_cleaned/weibo_train.csv')\n",
    "\n",
    "images=[]\n",
    "for image_name in file['image']:\n",
    "    images.append(image_name[:-7]) \n",
    "set(images),print(len(file['event_label'].unique()))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "12\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "({'bringback_fake',\n",
       "  'columbianChemicals_fake',\n",
       "  'eclipse',\n",
       "  'elephant_fake',\n",
       "  'garissa',\n",
       "  'livr',\n",
       "  'passport',\n",
       "  'pigFish',\n",
       "  'samurai',\n",
       "  'sandy_fake',\n",
       "  'sandy_real',\n",
       "  'sandy_real_',\n",
       "  'sochi_fak',\n",
       "  'sochi_fake',\n",
       "  'underwater_fake'},\n",
       " None)"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.11 64-bit ('eann': conda)"
  },
  "interpreter": {
   "hash": "465fa90a28663a9eacb4f6b007ef9489041ea648cb9c03179d70018ef7b6160c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}