{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import seaborn as sns\n",
    "\n",
    "def load_tweets_data(name): \n",
    "    csv_path = os.path.join(\"./mediaeval2016/\", name) \n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "trainTweets = load_tweets_data(\"train_posts.csv\")\n",
    "\n",
    "\n",
    "# 转小写\n",
    "trainTweets['post_text'] = trainTweets['post_text'].apply(lambda x: x.lower())\n",
    "trainTweets.head()\n",
    "\n",
    "# 删除标点\n",
    "def punctuation_removal(text):\n",
    "    all_list = [char for char in text if char not in string.punctuation]\n",
    "    clean_str = ''.join(all_list)\n",
    "    return clean_str\n",
    "\n",
    "trainTweets['post_text'] = trainTweets['post_text'].apply(punctuation_removal)\n",
    "trainTweets.head()\n",
    "\n",
    "# 删除英文停用词\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "trainTweets['post_text'] = trainTweets['post_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "\n",
    "trainTweets.head()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/zhongshannan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>post_text</th>\n",
       "      <th>user_id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>username</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>324597532548276224</td>\n",
       "      <td>dont need feds solve bostonbombing 4chan httpt...</td>\n",
       "      <td>886672620</td>\n",
       "      <td>boston_fake_03</td>\n",
       "      <td>SantaCruzShred</td>\n",
       "      <td>Wed Apr 17 18:57:37 +0000 2013</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>325145334739267584</td>\n",
       "      <td>pic comparison boston suspect sunil tripathis ...</td>\n",
       "      <td>21992286</td>\n",
       "      <td>boston_fake_23</td>\n",
       "      <td>Oscar_Wang</td>\n",
       "      <td>Fri Apr 19 07:14:23 +0000 2013</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>325152091423248385</td>\n",
       "      <td>im completely convinced sunil tripathi fellow—...</td>\n",
       "      <td>16428755</td>\n",
       "      <td>boston_fake_34</td>\n",
       "      <td>jamwil</td>\n",
       "      <td>Fri Apr 19 07:41:14 +0000 2013</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>324554646976868352</td>\n",
       "      <td>brutal lo que se puede conseguir en colaboraci...</td>\n",
       "      <td>303138574</td>\n",
       "      <td>boston_fake_03</td>\n",
       "      <td>rubenson80</td>\n",
       "      <td>Wed Apr 17 16:07:12 +0000 2013</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>324315545572896768</td>\n",
       "      <td>4chan bombing throwing httptcodiyso7lxqm httpt...</td>\n",
       "      <td>180460772</td>\n",
       "      <td>boston_fake_15</td>\n",
       "      <td>Slimlenny</td>\n",
       "      <td>Wed Apr 17 00:17:06 +0000 2013</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              post_id                                          post_text  \\\n",
       "0  324597532548276224  dont need feds solve bostonbombing 4chan httpt...   \n",
       "1  325145334739267584  pic comparison boston suspect sunil tripathis ...   \n",
       "2  325152091423248385  im completely convinced sunil tripathi fellow—...   \n",
       "3  324554646976868352  brutal lo que se puede conseguir en colaboraci...   \n",
       "4  324315545572896768  4chan bombing throwing httptcodiyso7lxqm httpt...   \n",
       "\n",
       "     user_id        image_id        username                       timestamp  \\\n",
       "0  886672620  boston_fake_03  SantaCruzShred  Wed Apr 17 18:57:37 +0000 2013   \n",
       "1   21992286  boston_fake_23      Oscar_Wang  Fri Apr 19 07:14:23 +0000 2013   \n",
       "2   16428755  boston_fake_34          jamwil  Fri Apr 19 07:41:14 +0000 2013   \n",
       "3  303138574  boston_fake_03      rubenson80  Wed Apr 17 16:07:12 +0000 2013   \n",
       "4  180460772  boston_fake_15       Slimlenny  Wed Apr 17 00:17:06 +0000 2013   \n",
       "\n",
       "  label  \n",
       "0  fake  \n",
       "1  fake  \n",
       "2  fake  \n",
       "3  fake  \n",
       "4  fake  "
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import jieba\n",
    "\n",
    "# 自定义图层颜色\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "colors = [\"#000000\", \"#111111\", \"#101010\", \"#121212\", \"#212121\", \"#222222\"]\n",
    "cmap = LinearSegmentedColormap.from_list(\"mycmap\", colors)\n",
    "\n",
    "def generate_word_cloud(input_tweets):\n",
    "    tweets = input_tweets\n",
    "    tweets_text = ' '.join(jieba.cut(tweets.text))\n",
    "    # tweets_text = \" \".join(tweets.text.to_numpy().tolist())\n",
    "    print(tweets.post_text)\n",
    "    \n",
    "    # 词云\n",
    "    # wordcloud = WordCloud(width = 3000, height = 2000, random_state=1, background_color='White', colormap=cmap, collocations=False).generate(tweets_text)\n",
    "    # plt.figure(figsize=(16,10))\n",
    "    # plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    # plt.axis('off') # 关闭轴\n",
    "    # plt.show()\n",
    "    # wordcloud.to_file(\"1.png\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "generate_word_cloud(trainTweets[trainTweets['label'] == 'real'])"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'text'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgenerate_word_cloud\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainTweets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrainTweets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreal\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36mgenerate_word_cloud\u001b[0;34m(input_tweets)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_word_cloud\u001b[39m(input_tweets):\n\u001b[1;32m     11\u001b[0m     tweets \u001b[38;5;241m=\u001b[39m input_tweets\n\u001b[0;32m---> 12\u001b[0m     tweets_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(jieba\u001b[38;5;241m.\u001b[39mcut(\u001b[43mtweets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[38;5;241m.\u001b[39mto_numpy()\u001b[38;5;241m.\u001b[39mtolist()))\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# tweets_text = \" \".join(tweets.text.to_numpy().tolist())\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(tweets\u001b[38;5;241m.\u001b[39mpost_text)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/Py38/lib/python3.8/site-packages/pandas/core/generic.py:5583\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   5577\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[1;32m   5578\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[1;32m   5579\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[1;32m   5580\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[1;32m   5581\u001b[0m ):\n\u001b[1;32m   5582\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[0;32m-> 5583\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'text'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('Py38': conda)"
  },
  "interpreter": {
   "hash": "62792cbd3b5bf2807c49140b9dd77f3574e6f3863eef786f6f586b1ba9747cfd"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}